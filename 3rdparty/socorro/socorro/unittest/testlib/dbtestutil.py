"""
Refactor the stuff that sets up and tears down the test database details
"""
import datetime as dt
import logging
import time
import psycopg2
import socorro.unittest.testlib.createJsonDumpStore as createJDS

def datetimeNow(cursor):
  cursor.execute("SELECT LOCALTIMESTAMP(6)")
  return cursor.fetchone()[0]

def fillProcessorTable(cursor, processorCount, stamp=None, processorMap = {},logger = None):
  """
  Puts some entries into the processor table.
  Also creates priority_jobs_NNN for each processor id, unless that table exists
  Given a map of id->timestamp, sets the lastseendatetime for each successive processor to that stamp
  (Ignores ids generated by the count or in the processorMap, and uses database's serial id generator)
  """
  if not logger:
    logger = logging.getLogger()
    
  if not stamp: stamp = dt.datetime.now()
  if not processorCount and not processorMap: return
  sql = "INSERT INTO processors (name,startdatetime,lastseendatetime) VALUES (%s,%s,%s);"
  data = []
  if processorMap:
    data.extend([('test_%d'%(id),stamp,processorMap.get(id,stamp)) for id in processorMap.keys() ])
  else:
    data.extend([('test_%d'%(x),stamp, stamp) for x in range(1,processorCount+1) ])
  cursor.executemany(sql,data)
  cursor.connection.commit()

  sql = "SELECT id from processors;"
  cursor.execute(sql)
  allIds = cursor.fetchall()
  sql = "CREATE TABLE priority_jobs_%s (uuid varchar(50) not null primary key);"
  for tup in allIds:
    try:
      cursor.execute(sql%(tup[0]))
      cursor.connection.commit()
    except psycopg2.ProgrammingError:
      cursor.connection.rollback()

def moreUuid():
  data = [ x for x in createJDS.jsonFileData.keys() ] # fixed order
  jdsIndex = 0
  hex4 = 'dead'
  currentHex = 0
  while True:
    if 0xffff == currentHex: currentHex = 0
    if jdsIndex >= len(data):
      jdsIndex = 0
      currentHex += 1
      if 0xdead == currentHex : currentHex += 1
      hex4 = "%04x"%currentHex
    yield data[jdsIndex].replace('dead',hex4)
    jdsIndex += 1

def makeJobDetails(idsMapToCounts):
  """
  Generate some bogus uuid and path data
  """
  data = []
  gen = moreUuid()
  for id in idsMapToCounts.keys():
    for x in range(idsMapToCounts[id]):
      uuid = gen.next()
      data.append(("/hmm/%s/%s/%s"%(uuid[:2],uuid[2:4],uuid),uuid,id,))
  return data

def setPriority(cursor,jobIds,priorityTableName=None):
  """
  if priorityTableName: for each id in jobIds, insert row in that table holding the uuid of that job
  otherwise, set the job.priority column in the jobs table
    BEWARE: The job ids must be actual ids from the jobs table or this will fail
  """
  if not jobIds: return
  wherePart = 'WHERE id IN (%s)'%(', '.join((str(x) for x in jobIds)))
  if priorityTableName:
    sql = "INSERT INTO %s (uuid) SELECT uuid FROM jobs %s"%(priorityTableName,wherePart)
  else:
    sql = "UPDATE jobs SET priority = 1 %s"%(wherePart)
  cursor.execute(sql)
  cursor.connection.commit()

def addSomeJobs(cursor,idsMapToCounts, logger = None):
  """
  Insert the requested rows into jobs table.
  idsMapToCounts: id:countOfjobsForProcessorWithThisId
    BEWARE: The ids must be actual ids from the processors table or this will fail.
  returns
  """
  if not logger:
    logger = logging.getLogger()
  logger.debug("ADDING: %s"%(str(idsMapToCounts)))
  data = makeJobDetails(idsMapToCounts)
  sql = "INSERT INTO jobs (pathname,uuid,owner) VALUES (%s,%s,%s)"
  try:
    cursor.executemany(sql,data)
  except Exception,x:
    logger.error("Failed to addSomeJobs(%s): %s",str(idsMapToCounts),x)
    raise x
  cursor.connection.commit()
  return data

  
